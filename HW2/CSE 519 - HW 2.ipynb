{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "   #### Removing Non Numeric Data\n",
    "        Before finding correlation matrix and creating a heat map, we need to remove non-numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Data Shape :  (2985217, 56)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "properties_file = 'properties_2016.csv'\n",
    "data_reader = pd.read_csv(properties_file, dtype='str')\n",
    "\n",
    "del data_reader['propertycountylandusecode']\n",
    "del data_reader['propertyzoningdesc']\n",
    "\n",
    "x = data_reader.loc[:,'taxdelinquencyflag']\n",
    "x.loc[x == 'Y'] = 1\n",
    "x.loc[x != 1] = 0\n",
    "\n",
    "x = data_reader.loc[:,'hashottuborspa']\n",
    "x.loc[x == 'true'] = 1\n",
    "x.loc[x != 1] = 0\n",
    "\n",
    "x = data_reader.loc[:,'fireplaceflag']\n",
    "x.loc[x == 'true'] = 1\n",
    "x.loc[x != 1] = 0\n",
    "\n",
    "data_reader.to_csv('numeric.csv', mode='w', index = False)\n",
    "\n",
    "print('Remaining Data Shape : ', data_reader.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Correlation Matrix\n",
    "        After removing non-numeric data, I picked some interesting pairs of properties and created a correlation matrix.\n",
    "#### Visualization of Correlations using Heat Map\n",
    "    Use seaborn library to visualize the heatmap of correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "data_reader = pd.read_csv('numeric.csv')\n",
    "pearson_corr = data_reader.corr()\n",
    "\n",
    "corrs = [(11, 12), (11, 45), (11, 17), (11, 49), (11, 55), (11, 35), (11, 30), (11, 27), (11, 36), (11, 7), (42, 37), (42, 13), (42, 25), (42, 1), (42, 7), (42, 38), (42, 9), (42, 18), (42, 50), (42, 10), (34, 22), (34, 18), (34, 41), (34, 21), (34, 45), (34, 51), (34, 38), (34, 9), (34, 4), (34, 42), (54, 39), (54, 16), (54, 50), (54, 40), (54, 15), (54, 46), (54, 41), (54, 10), (54, 53), (54, 18), (31, 29), (31, 54), (31, 33), (31, 14), (31, 24), (31, 13), (31, 42), (31, 11), (31, 30), (31, 51), (17, 51), (17, 39), (17, 27), (17, 43), (17, 50), (17, 40), (17, 55), (17, 3), (17, 48), (17, 2), (4, 27), (4, 45), (4, 46), (4, 5), (4, 48), (4, 30), (4, 21), (4, 55), (4, 12), (4, 2), (6, 30), (6, 36), (6, 23), (6, 7), (6, 11), (6, 49), (6, 31), (6, 12), (6, 18), (6, 3), (13, 12), (13, 8), (13, 37), (13, 55), (13, 33), (13, 3), (13, 28), (13, 29), (13, 45), (13, 25), (53, 1), (53, 38), (53, 34), (53, 13), (53, 35), (53, 28), (53, 44), (53, 37), (53, 54), (53, 40)]\n",
    "\n",
    "for i in range(56):\n",
    "\tfor j in range(56):\n",
    "\t\tif tuple([i, j]) not in corrs:\n",
    "\t\t\tpearson_corr.loc[data_reader.columns[i], data_reader.columns[j]] = np.NaN\n",
    "\n",
    "dim = pearson_corr.shape\n",
    "to_drop_col = []\n",
    "for column in pearson_corr.columns:\n",
    "    nans = pearson_corr.loc[:, column].isnull().sum()\n",
    "    if nans == dim[0]:\n",
    "        to_drop_col.append(column)\n",
    "\n",
    "to_drop_row = []\n",
    "for index, row in pearson_corr.iterrows():\n",
    "    nans = pearson_corr.loc[index, :].isnull().sum()\n",
    "    if nans == dim[1]:\n",
    "        to_drop_row.append(index)\n",
    "\n",
    "pearson_corr = pearson_corr.drop(to_drop_col, axis=1)\n",
    "pearson_corr = pearson_corr.drop(to_drop_row, axis=0)\n",
    "\n",
    "plt.figure(figsize=(14, 5), dpi= 100)\n",
    "sns.heatmap(pearson_corr)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Highly Correlated: **calculatedfinishedsquarefeet** and **finishedsquarefeet12** has a perfect correlation with a pearson score of 1.\n",
    "- Highly Anti-Correlated: **calculatedfinishedsquarefeet** and **buildingclasstypeid** has correlation score of ~0.25 which tells that if expensive material is used in a house , finished area in that house is usually less which makes sense.\n",
    "- There are a bunch of pairs which are not really correlated. We can see those in the heatmap above\n",
    "#### Note: highest correlated and antirelated entities are picked from the subset of pairs of the items that I found interesting and are not the highest correlated and least correlated entities from all the pairs of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "#### Correlation of Properties with log-error\n",
    "It is interesting to note that some properties like **basementsqft** show decent correlation with log-error and some are anti-correlated like **heatingsystempeid**, while there are some properties like **latitude** and **roomcnt** that are neither correlated nor strongly anti-correlated. It shows that these features will not contribute much while trying to fit them on a linear model. and this those show close relation should be used to while fitting using linear models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = pd.read_csv('numeric.csv')\n",
    "log_err = pd.read_csv('train_2016_v2.csv')\n",
    "result = pd.merge(data_reader, log_err, on=['parcelid', 'parcelid'])\n",
    "result = result.drop('transactiondate', axis=1)\n",
    "result = result.drop('parcelid', axis=1)\n",
    "pearson_corr = result.corr()\n",
    "\n",
    "cols = pearson_corr.columns\n",
    "pearson_corr = pearson_corr.loc['logerror', :]\n",
    "pearson_corr1 = pearson_corr.values.reshape(1,56)\n",
    "pc1 = pd.DataFrame(pearson_corr1, columns=cols)\n",
    "pc1 = pc1.drop('logerror', axis=1)\n",
    "\n",
    "plt.figure(figsize=(14, 3), dpi= 100)\n",
    "sns.heatmap(pc1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variation in log-error w.r.t Time\n",
    "Looking at the following graphs, we see that log-error in the price prediction is not monotonic with the time. But, one interesting thing to note is that it shows a cyclic behavior.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_err = pd.read_csv('train_2016_v2.csv')\n",
    "log_err['transactiondate'] =pd.to_datetime(log_err.transactiondate)\n",
    "log_err = log_err.sort_values(by='transactiondate')\n",
    "log_err.plot(x='transactiondate', y='logerror',figsize=(17, 6))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freqeuncy Distribution of logerror\n",
    "After looking at the following plots, we infer that in most of the predictions, error was very low and there are very few outliers which can easily be filtered while doing regression. That's the beauty of log scale! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_err = pd.read_csv('train_2016_v2.csv')\n",
    "print(log_err['logerror'].min())\n",
    "print(log_err['logerror'].max())\n",
    "\n",
    "arr = log_err['logerror'].values\n",
    "\n",
    "buckets = [-4.5, -4, -3.5, -3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5]\n",
    "index = np.arange(19)\n",
    "counts = [0 for i in range(len(buckets))]\n",
    "mn = 100\n",
    "mx = -100\n",
    "for x in arr:\n",
    "    ind = int(( x + 4.605)*2.05)\n",
    "    \n",
    "    if ind > 18:\n",
    "        ind = 18\n",
    "    counts[ind] += 1\n",
    "\n",
    "plt.figure(figsize=(10, 2), dpi= 100)\n",
    "plt.bar(index, counts, 0.35)\n",
    "plt.xticks(index + 0.5 / 2, buckets)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zoomed Version :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3), dpi= 100)\n",
    "plt.bar(index, counts, 0.35)\n",
    "plt.xticks(index + 0.5 / 2, buckets)\n",
    "plt.ylim([0,1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Room Count vs Bath Count\n",
    "One expects Full Bath Count to increase with the number of rooms, but surprisingly, in the following graph, it shows that its difficult to predict the number of full baths a house can have given the number of rooms it has. This preposition has on assumption that this data has no outliers. Since, we are not considering frequency of a certain count of bathrooms with respect to rooms here. We might be able to give a stronger proposition if we look into the freqeuncies as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 100000000\n",
    "data = pd.read_csv('numeric.csv')\n",
    "data = data[data['roomcnt'].notnull()]\n",
    "data = data[data['fullbathcnt'].notnull()]\n",
    "data = data[data['roomcnt']!=0]\n",
    "# x = data['roomcnt'].unique())\n",
    "# print(data.shape)\n",
    "data = data.loc[:50000, :]\n",
    "# print(data['roomcnt'].unique())\n",
    "# print(data['fullbathcnt'].unique())\n",
    "data.plot(x='roomcnt', y='fullbathcnt', style='o', xlim=[5,14], ylim=[0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spread of Features w.r.t. Log Error\n",
    "In this, we plot 7 variables with log_error to see how the spread of different variables look like. It looks like spread of most of the variables is similar in such a way that no matter you apply linear regression with any of these (one variable at a time) you will be getting almost the same fit. Part 3 confirms this hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = pd.read_csv('numeric.csv')\n",
    "properties = properties.fillna(properties.mean())\n",
    "log_err = pd.read_csv('train_2016_v2.csv')\n",
    "result = pd.merge(properties, log_err, on=['parcelid', 'parcelid'])\n",
    "count = 0\n",
    "\n",
    "y = result['logerror'].values\n",
    "y = y.reshape(90275, 1)\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi= 100)\n",
    "\n",
    "for col in result.columns:\n",
    "        if col == 'parcelid' or col == 'logerror' or col == 'transactiondate':\n",
    "            continue\n",
    "        if count >= 7:\n",
    "            break\n",
    "        x = result[col].values\n",
    "        x = x.reshape(90275, 1)\n",
    "        plt.scatter(x, y, label=col, alpha=0.2)\n",
    "        plt.legend(loc='lower right')\n",
    "        count += 1\n",
    "plt.ylabel('logerror')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is a bit difficult to see all the variables in the above one plot. For ease, here are all the plots spearately: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "properties = pd.read_csv('numeric.csv')\n",
    "properties = properties.fillna(properties.mean())\n",
    "log_err = pd.read_csv('train_2016_v2.csv')\n",
    "result = pd.merge(properties, log_err, on=['parcelid', 'parcelid'])\n",
    "count = 0\n",
    "\n",
    "y = result['logerror'].values\n",
    "y = y.reshape(90275, 1)\n",
    "\n",
    "\n",
    "\n",
    "for col in result.columns:\n",
    "        if col == 'parcelid' or col == 'logerror' or col == 'transactiondate':\n",
    "            continue\n",
    "        if count >= 7:\n",
    "            break\n",
    "        x = result[col].values\n",
    "        x = x.reshape(90275, 1)\n",
    "        plt.figure(figsize=(15, 5), dpi= 100)\n",
    "        plt.scatter(x, y, label=col, alpha=0.2)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.ylabel('logerror')\n",
    "        count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "    To apply linear regression, first step is to get rid of NaN values. To start simple, just replace all the NaN values with there averages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from copy import deepcopy\n",
    "\n",
    "properties = pd.read_csv('numeric.csv')\n",
    "properties = properties.fillna(properties.mean())\n",
    "log_err = pd.read_csv('train_2016_v2.csv')\n",
    "result = pd.merge(properties, log_err, on=['parcelid', 'parcelid'])\n",
    "result = result.drop('transactiondate', axis=1)\n",
    "result = result.drop('parcelid', axis=1)\n",
    "result.to_csv('simplemodel_data.csv', index=False, mode='w')\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Using the Above data, apply a simple linear regression on single variables and pick the 10 variables for which the linear regression gives the lowest mean squared error. Use those variables togethher to further reduce the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('simplemodel_data.csv')\n",
    "y = result['logerror'].values\n",
    "y = y.reshape(90275, 1)\n",
    "y_train = y[:-20000]\n",
    "y_test = y[-20000:]\n",
    "\n",
    "data = data.loc[:, data.columns != 'logerror']\n",
    "\n",
    "err_list = []\n",
    "properties = dict()\n",
    "\n",
    "for column in  data.columns:\n",
    "    x = result[column].values\n",
    "    x = x.reshape(90275, 1)\n",
    "    x_train = x[:-20000]\n",
    "    x_test = x[-20000:]\n",
    "    \n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x_train, y_train)\n",
    "    y_pred = regr.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    err_list.append(mse)\n",
    "    properties[mse] = column\n",
    "\n",
    "err_list.sort()\n",
    "\n",
    "err_list = err_list[:10]\n",
    "\n",
    "\n",
    "imp_properties = []\n",
    "for err in err_list:\n",
    "    imp_properties.append(properties[err])\n",
    "\n",
    "print(\"Most Important Properties : \", imp_properties)\n",
    "\n",
    "# data = data.loc[:, imp_properties]\n",
    "x = data.values\n",
    "x_train = x[:-20000]\n",
    "x_test = x[-20000:]\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x_train, y_train)\n",
    "y_pred = regr.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Square Error of Simple Model with all shortlisted features : ', mse)\n",
    "\n",
    "data = data.loc[:, imp_properties]\n",
    "x = data.values\n",
    "x_train = x[:-20000]\n",
    "x_test = x[-20000:]\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x_train, y_train)\n",
    "y_pred = regr.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Square Error of Simple Model with top 10 features : ', mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error of Simple Model : 0.0249462576559\n",
    "\n",
    "Following Entries Look like Most Important Properites based on Mean Squared Error for Linear Regression:\n",
    "- **finishedsquarefeet12**\n",
    "- **calculatedfinishedsquarefeet**\n",
    "- **bathroomcnt**\n",
    "- **calculatedbathnbr**\n",
    "- **fullbathcnt**\n",
    "- **taxdelinquencyflag**\n",
    "- **bedroomcnt**\n",
    "- **yearbuilt**\n",
    "- **structuretaxvaluedollarcnt**\n",
    "- **heatingorsystemtypeid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "   #### Random Forests\n",
    "        First try a Random Forests on the same data-set without cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "data = pd.read_csv('simplemodel_data.csv')\n",
    "y = result['logerror'].values\n",
    "y = y.reshape(90275, 1)\n",
    "y_train = y[:-20000]\n",
    "y_test = y[-20000:]\n",
    "data = data.loc[:, data.columns != 'logerror']\n",
    "\n",
    "err_list = []\n",
    "properties = dict()\n",
    "\n",
    "for column in  data.columns:\n",
    "    x = result[column].values\n",
    "    x = x.reshape(90275, 1)\n",
    "    x_train = x[:-20000]\n",
    "    x_test = x[-20000:]\n",
    "    \n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    regr.fit(x_train, y_train.ravel())\n",
    "    y_pred = regr.predict(x_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    err_list.append(mse)\n",
    "    properties[mse] = column\n",
    "\n",
    "err_list.sort()\n",
    "\n",
    "err_list = err_list[:10]\n",
    "\n",
    "\n",
    "imp_properties = []\n",
    "for err in err_list:\n",
    "    imp_properties.append(properties[err])\n",
    "\n",
    "\n",
    "data = data.loc[:, imp_properties]\n",
    "x = data.values\n",
    "x_train = x[:-20000]\n",
    "x_test = x[-20000:]\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr.fit(x_train, y_train.ravel())\n",
    "y_pred = regr.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error of RandomForest Regression: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #### Nearest Nieghbors\n",
    "        Lets try Nearest Nieghbors regression on the same data-set without any significant preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "data = pd.read_csv('simplemodel_data.csv')\n",
    "print(data.shape)\n",
    "y = result['logerror'].values\n",
    "y = y.reshape(90275, 1)\n",
    "y_train = y[:-20000]\n",
    "y_test = y[-20000:]\n",
    "data = data.loc[:, data.columns != 'logerror']\n",
    "\n",
    "err_list = []\n",
    "properties = dict()\n",
    "\n",
    "for column in  data.columns:\n",
    "    x = result[column].values\n",
    "    x = x.reshape(90275, 1)\n",
    "    x_train = x[:-20000]\n",
    "    x_test = x[-20000:]\n",
    "    \n",
    "    regr = KNeighborsRegressor(n_neighbors=10)\n",
    "    regr.fit(x_train, y_train)\n",
    "    y_pred = regr.predict(x_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    err_list.append(mse)\n",
    "    properties[mse] = column\n",
    "\n",
    "err_list.sort()\n",
    "\n",
    "err_list = err_list[:10]\n",
    "\n",
    "\n",
    "imp_properties = []\n",
    "for err in err_list:\n",
    "    imp_properties.append(properties[err])\n",
    "\n",
    "\n",
    "data = data.loc[:, imp_properties]\n",
    "x = data.values\n",
    "x_train = x[:-20000]\n",
    "x_test = x[-20000:]\n",
    "regr = KNeighborsRegressor(n_neighbors=10)\n",
    "regr.fit(x_train, y_train)\n",
    "y_pred = regr.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error of Nearest Neighbor Regression: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "####   Removing Bad Properties and Filling NaNs\n",
    "  - Remove the Columns for which there are more than ~80% NaN Values because even if we fill those properties with anything, its going to effect regression negatively because of a lot of missing data. Also since, we have a decent number of properties to play with. We can afford get rid of some columns which are not giving significant information.\n",
    "  - After that, remove some columns into which filling anything like average or a zero value does not make sense e.g. bath-room count or pool count of a home.\n",
    "  - Next step is getting rid of all the remianing NaNs. Now, for better cleaning, its a good idea to have a look at each property and try to understand what this field represents and fill the missing values with something which makes more sense as compared to filling with just the mean of the column or zeros. Following is the list of properties for which I did this some-what more guided cleaning:\n",
    "   - **airconditioningtypeid** : From the Data dictionary, 6 indicates some unknown type, fill missing values with '6'\n",
    "   - **poolcnt** : Assume that if pool count of a home is NaN, it means that it don't have a pool. Fill missing values with zeros. (Same for **pooltypeid10**, **pooltypeid2**, **pooltypeid7**, **fireplacecnt**)\n",
    "   - **heatingorsystemtypeid** : From the Data dictionary, 14 indicates some unknown type, fill missing values with '14'\n",
    "   - **buildingqualitytypeid** : Since there were not many houses with missing building quality. So, I just assigned mean value for the missing ones. (Same for **garagetotalsqft**)\n",
    "  - After this, I just removed all the rows in which there were still some missing values. After doing all of this, i ended up with almost ~7000 entries and ~35 properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization of Data\n",
    "  Normalize all the properties such that all of them have a mean of zero and standard deviation of 1. This is really helpful in doing regression if we want to treat all proprties equally. This helps to make the values in all the columns comaprable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = pd.read_csv('numeric.csv')\n",
    "log_err = pd.read_csv('train_2016_v2.csv')\n",
    "result = pd.merge(properties, log_err, on=['parcelid', 'parcelid'])\n",
    "\n",
    "to_remove = ['assessmentyear', 'numberofstories','unitcnt' ,'regionidneighborhood','garagecarcnt','architecturalstyletypeid', \n",
    "            'basementsqft', 'buildingclasstypeid', 'decktypeid','finishedfloor1squarefeet', 'finishedsquarefeet13',\n",
    "            'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6', 'poolsizesum', 'storytypeid',\n",
    "            'threequarterbathnbr', 'typeconstructiontypeid', 'yardbuildingsqft17', 'yardbuildingsqft26',\n",
    "            'taxdelinquencyyear', 'transactiondate']\n",
    "\n",
    "result = result.drop(to_remove, axis=1)\n",
    "\n",
    "tmp = result.loc[:,'airconditioningtypeid']\n",
    "tmp.fillna(6, inplace=True)\n",
    "\n",
    "tmp = result.loc[:,'poolcnt']\n",
    "tmp.fillna(0, inplace=True)\n",
    "\n",
    "tmp = result.loc[:,'pooltypeid10']\n",
    "tmp.fillna(0, inplace=True)\n",
    "\n",
    "tmp = result.loc[:,'pooltypeid2']\n",
    "tmp.fillna(0, inplace=True)\n",
    "\n",
    "tmp = result.loc[:,'pooltypeid7']\n",
    "tmp.fillna(0, inplace=True)\n",
    "\n",
    "tmp = result.loc[:,'fireplacecnt']\n",
    "tmp.fillna(0, inplace=True)\n",
    "\n",
    "tmp = result.loc[:,'heatingorsystemtypeid']\n",
    "tmp.fillna(14, inplace=True)\n",
    "\n",
    "tmp = result.loc[:,'buildingqualitytypeid']\n",
    "tmp.fillna(int(tmp.mean()), inplace=True)\n",
    "\n",
    "tmp = result.loc[:,'garagetotalsqft']\n",
    "tmp.fillna(int(tmp.mean()), inplace=True)\n",
    "\n",
    "result = result.dropna()\n",
    "\n",
    "print(result.isnull().sum().sum())\n",
    "print(result.shape)\n",
    "\n",
    "result.to_csv('clean_data.csv', mode='w', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test All Three Regression Techniques Again\n",
    "Now, lets test all the three techniques again and see what improvements we get and find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "for regression in ['linear', 'rf', 'kNN']:\n",
    "    \n",
    "\n",
    "    data = pd.read_csv('clean_data.csv')\n",
    "    data = data.drop('parcelid', axis=1)\n",
    "    y = result['logerror'].values\n",
    "    y = y.reshape(73972, 1)\n",
    "    y_train = y[:-10000]\n",
    "    y_test = y[-10000:]\n",
    "    data = data.loc[:, data.columns != 'logerror']\n",
    "    \n",
    "    data = (data - data.mean())/(data.std(ddof=0))\n",
    "\n",
    "    err_list = []\n",
    "    properties = dict()\n",
    "\n",
    "    for column in  data.columns:\n",
    "        x = result[column].values\n",
    "        x = x.reshape(73972, 1)\n",
    "        x_train = x[:-10000]\n",
    "        x_test = x[-10000:]\n",
    "        \n",
    "        regr = None\n",
    "        if regression == 'linear':\n",
    "            regr = linear_model.LinearRegression()\n",
    "        elif regression == 'rf':\n",
    "            regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "        elif regression == 'kNN':\n",
    "            regr = KNeighborsRegressor(n_neighbors=12)\n",
    "        regr.fit(x_train, y_train.ravel())\n",
    "        \n",
    "        y_pred = regr.predict(x_test)\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        err_list.append(mse)\n",
    "        properties[mse] = column\n",
    "\n",
    "    err_list.sort()\n",
    "\n",
    "    err_list = err_list[:10]\n",
    "\n",
    "\n",
    "    imp_properties = []\n",
    "    for err in err_list:\n",
    "        imp_properties.append(properties[err])\n",
    "\n",
    "\n",
    "    data = data.loc[:, imp_properties]\n",
    "    x = data.values\n",
    "    x_train = x[:-10000]\n",
    "    x_test = x[-10000:]\n",
    "    regr = None\n",
    "    if regression == 'linear':\n",
    "        regr = linear_model.LinearRegression()\n",
    "    elif regression == 'rf':\n",
    "        regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    elif regression == 'kNN':\n",
    "        regr = KNeighborsRegressor(n_neighbors=12)\n",
    "    regr.fit(x_train, y_train.ravel())\n",
    "    y_pred = regr.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print('Mean Squared Error of %s Regression: ' % regression, mse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "I tested following Models and measured Mean Squared Error (MSE) in all of them:\n",
    "\n",
    "- Linear Regression with Simple Cleaning of Data using top 10 features (LR1)\n",
    "- Random Forest with Simple Cleaning of Data using top 10 features (RF1)\n",
    "- Nearest Neighbors with Simple Cleaning of Data using top 10 features (kNN1)\n",
    "- Linear Regression with Detailed Cleaning of Data using top 10 features (LR2)\n",
    "- Random Forest with Detailed Cleaning of Data using top 10 features (RF2)\n",
    "- Nearest Neighbors with Detailed Cleaning of Data using top 10 features (kNN1)\n",
    "- Nearest Neighbors with Simple Cleaning of Data using all features which made sense to me(~35) (LR3)\n",
    "\n",
    "Following Table shows the comparison of MSE of all these models:\n",
    "\n",
    "| Model     | MSE             |\n",
    "|-----------|:---------------:|\n",
    "| LR1       | 0.0249462576559 |\n",
    "| RR1       | 0.0248379786825 |\n",
    "| kNN1      | 0.0349486639042 |\n",
    "| **LR2**       | **0.0224173797393** |\n",
    "| RR2       | 0.0226265810949 |\n",
    "| kNN2      | 0.0241265817777 |\n",
    "| LR3       | 0.02503610659   |\n",
    "\n",
    "**LR2** performs best among these following are the plausible reasons for improvement as compared to others:\n",
    "- Getting rid of columns which don't have significant amount of values don't force the model to converge to highly unpredicted and some what less probable state.\n",
    "- Dealing with NaNs in a more detailed fashion as compared to just filling with NaNs\n",
    "- With Simple cleaning, performance of Random Forests is similar to Linear Regression, but it doesnot improve with cleaning to that extent. It looks like LR benefits more from cleaning as compared to RF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "My Submission Score :\n",
    "My submission Username: uhafeez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Hello World!')\n",
    "# properties_file = 'properties_2016.csv'\n",
    "# data_reader = pd.read_csv(properties_file, dtype='str')\n",
    "# print(data_reader.shape)\n",
    "\n",
    "# properties_file = 'sample_submission.csv'\n",
    "# data_reader = pd.read_csv(properties_file, dtype='str')\n",
    "# print(data_reader.shape)\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('clean_data.csv')\n",
    "data = data.drop('parcelid', axis=1)\n",
    "y = result['logerror'].values\n",
    "y = y.reshape(73972, 1)\n",
    "y_train = y[:-10000]\n",
    "y_test = y[-10000:]\n",
    "data = data.loc[:, data.columns != 'logerror']\n",
    "\n",
    "data = (data - data.mean())/(data.std(ddof=0))\n",
    "\n",
    "err_list = []\n",
    "properties = dict()\n",
    "\n",
    "for column in  data.columns:\n",
    "    x = result[column].values\n",
    "    x = x.reshape(73972, 1)\n",
    "    x_train = x[:-10000]\n",
    "    x_test = x[-10000:]\n",
    "\n",
    "    regr = None\n",
    "    if regression == 'linear':\n",
    "        regr = linear_model.LinearRegression()\n",
    "    elif regression == 'rf':\n",
    "        regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    elif regression == 'kNN':\n",
    "        regr = KNeighborsRegressor(n_neighbors=12)\n",
    "    regr.fit(x_train, y_train.ravel())\n",
    "\n",
    "    y_pred = regr.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    err_list.append(mse)\n",
    "    properties[mse] = column\n",
    "\n",
    "err_list.sort()\n",
    "\n",
    "err_list = err_list[:10]\n",
    "\n",
    "\n",
    "imp_properties = []\n",
    "for err in err_list:\n",
    "    imp_properties.append(properties[err])\n",
    "\n",
    "print(imp_properties)\n",
    "\n",
    "x_test1 = pd.read_csv('numeric.csv')\n",
    "x_test = x_test1.fillna(x_test1.mean())\n",
    "x_test = x_test.loc[:, imp_properties]\n",
    "print(x_test.shape)\n",
    "\n",
    "data = data.loc[:, imp_properties]\n",
    "x = data.values\n",
    "x_train = x\n",
    "y_train = y\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x_train, y_train.ravel())\n",
    "y_pred = regr.predict(x_test)\n",
    "print(type(y_pred))\n",
    "y_pred = y_pred.reshape(2985217, 1)\n",
    "ids = x_test1['parcelid'].values.reshape(2985217, 1)\n",
    "sub_file = 'sample_submission.csv'\n",
    "df = pd.read_csv(sub_file)\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "ind = 0\n",
    "ids = pd.DataFrame(ids)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "sub = pd.concat([ids, y_pred, y_pred, y_pred, y_pred, y_pred, y_pred], axis=1)\n",
    "sub.columns = ['ParcelId', '201610', '201611', '201612', '201710', '201711', '201712']\n",
    "# print(type(sub))\n",
    "# print(sub)\n",
    "# print(sub.shape)\n",
    "# sub = pd.DataFrame([ids, y_pred, y_pred, y_pred, y_pred, y_pred, y_pred], columns=['ParcelId', '201610', '201611', '201612', '201710', '201711', '201712'])\n",
    "\n",
    "\n",
    "# for pid in ids:\n",
    "#     df.loc[df.ParcelId == pid, '201610'] = y_pred[ind]\n",
    "#     df.loc[df.ParcelId == pid, '201611'] = y_pred[ind]\n",
    "#     df.loc[df.ParcelId == pid, '201612'] = y_pred[ind]\n",
    "#     df.loc[df.ParcelId == pid, '201710'] = y_pred[ind]\n",
    "#     df.loc[df.ParcelId == pid, '201711'] = y_pred[ind]\n",
    "#     df.loc[df.ParcelId == pid, '201712'] = y_pred[ind]\n",
    "#     ind += 1\n",
    "#     if ind % 10000 == 0:\n",
    "#         print(ind)\n",
    "\n",
    "sub.to_csv('submission.csv', index=False, mode='w')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
